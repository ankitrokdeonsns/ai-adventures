{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Source:\n",
    "* Andrew Ng ML [course](https://www.coursera.org/learn/machine-learning/)\n",
    "* Tom Mithchell ML [course]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximation\n",
    "Given a set of inputs $X$ and a set of corresponding outputs $Y$ we want to learn the unknown function $f: X \\to Y$\n",
    "\n",
    "To do this we build a set of hypothesis functions $H = \\{ h | h: X \\to Y \\}$.\n",
    "We choose the hypothesis $h \\in H$ that best approximates $f$\n",
    "\n",
    "### How do we build hypothesis functions?\n",
    "### How to decide which hypothesis function is better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use applicable algorithms for regression?\n",
    "### What are the assumptions that each ML algorithm makes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### hypothesis representation\n",
    "The goal of a classification algorithm is to predict the correct class label $y$ for a given feature vector $x$.\n",
    "In a binary classification problem we want to classify a given feature vector $x$ in 2 classes positive or negative.\n",
    "Since computers do not have a sense of *positive* or *negative* we represent these classes using numbers $1$ and $0$ respectively.\n",
    "\n",
    "As a simple approach we want to know how much importance we should give to each feature $x_{i}$ in $x$ such that we can correctly classify $x$.\n",
    "We denote the importance by $w$ the weight vector where $w_{i}$ is the importance of $x_{i}$.\n",
    "Since $x$ and $w$ are vectors with real numbers $w^T . x$ will be a real number.\n",
    "\n",
    "Since we want to classify $x$ in 2 classes we want to know some good threshold $t$ such that if $w^T . x >= t$ then we classify $x$ as $1$ otherwise $0$.\n",
    "\n",
    "It is very hard to decide the value of $t$ since $w^T . x$ can lie in $(-\\infty, \\infty)$\n",
    "It would be better if we can limit the range of $w^T . x$.\n",
    "\n",
    "We know a function $logistic(z) = \\frac{1}{1 + e^{-z}}$ which takes a real number $z$ as input and returns a value in the range $[0, 1]$.\n",
    "Hence $logistic(w^T . x)$ will be a value in the range $[0, 1]$.\n",
    "This value $logistic(w^T . x)$ can be thought of as the probability of $x$ of being an example of class $1$.\n",
    "\n",
    "Thus we represent the hypothesis function $h(x, w)$ as:\n",
    "\\begin{equation}\n",
    "    h(x, w) = logistic(w^T . x)\n",
    "\\end{equation}\n",
    "\n",
    "Now we know $h(x, w)$ will be a value in the range $[0, 1]$ we can classify $x$ using a simple threshold of $0.5$.\n",
    "Thus if:\n",
    "\\begin{equation}\n",
    "    h(x, w) >= 0.5\n",
    "\\end{equation}\n",
    "we classify $x$ as class $1$ otherwise class $0$.\n",
    "\n",
    "### decision boundary\n",
    "We chose the threshold after using the $sigmoid$ function.\n",
    "Equivalently we can say if:\n",
    "\\begin{equation}\n",
    "    w^T . x >= 0\n",
    "\\end{equation}\n",
    "we classify $x$ as class $1$ otherwise class $0$.\n",
    "This defines the decision boundary with which we can classify $x$ in a deterministic fashion.\n",
    "Since we are using *logistic* function as our hypothesis this method of classification is **Logistic Regression**.\n",
    "\n",
    "### cost function\n",
    "We need to know how well our hypothesis performs during prediction.\n",
    "If an example $x$ has class label as $1$ then we want $h(x, w)$ to be a value as close to $1$ as possible.\n",
    "Similarly, if an example $x$ has class label as $0$ then we want $h(x, w)$ to be a value as close to $0$ as possible.\n",
    "If our hypothesis is incorrect then we want to represent the same with a high cost value.\n",
    "We model this cost function based on *log* transformations of our hypothesis function.\n",
    "This gives us a convex function which has a unique global minima.\n",
    "We represent the cost function for **Logistic Regression** as follows:\n",
    "\\begin{equation}\n",
    "    cost(x, w, y) = - ylog(h(x, w)) - (1 - y)log(1 - h(x, w))\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "In logistic regression for class `1` we want $\\theta^Tx \\gg 0$.\n",
    "Similarly for class `0` we want we want $\\theta^Tx \\ll 0$.\n",
    "\n",
    "We consider loss function for `Logistic Regression`\n",
    "\n",
    "$\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m} y^{(i)}\\log(h_\\theta(x^{(i)}) + (1- y^{(i)})\\log(1 - h_\\theta(x^{(i)}) + \\frac{\\lambda}{2m}\\displaystyle\\sum_{j=1}^{n} \\theta^2$ \n",
    "\n",
    "where $h_\\theta(x) = sigmoid(\\theta^Tx)$\n",
    "\n",
    "We approximate $\\log(h_\\theta(x))$ for class `1` and $\\log(1 - h_\\theta(x))$ for class `0` by piecewise linear functions $cost_{1}(h_\\theta(x^{(i)})$ and $cost_{0}(h_\\theta(x^{(i)})$ respectively. So the loss function looks as follows\n",
    "\n",
    "$\\frac{C}{m}\\displaystyle\\sum_{i=1}^{m} y^{(i)}cost_{1}(h_\\theta(x^{(i)}) + (1- y^{(i)})cost_{0}(h_\\theta(x^{(i)}) + \\frac{1}{2m}\\displaystyle\\sum_{j=1}^{n} \\theta^2$ \n",
    "where \n",
    "\n",
    "$h_\\theta(x) = 1$ if $\\theta^Tx >= 0$\n",
    "\n",
    "$h_\\theta(x) = 0$ if $\\theta^Tx < 0$\n",
    "\n",
    "$C$ is a regularization parameter as a counterpart to $\\lambda$\n",
    "\n",
    "The cost functions are such that if $y = 1$ then we want $\\theta^Tx >= 1$ and if $y = 0$ we want $\\theta^Tx <= -1$\n",
    "in contrast to the logistic regression decision criteria as $\\theta^Tx >= 0$ if $y = 1$ and $\\theta^Tx < 0$ if $y = 0$  \n",
    "\n",
    "For our classification to work well we need the main part of the loss function to be 0.\n",
    "To achieve this we set a very high value for $C$.\n",
    "Hence for $y = 1$ we need $\\theta^Tx >= 1$ and for $y = 0$ we need $\\theta^Tx <= -1$.\n",
    "The final objective function looks as:\n",
    "\n",
    "$min_{\\theta} \\frac{1}{2m}\\displaystyle\\sum_{j=1}^{n} \\theta^2$\n",
    "\n",
    "such that\n",
    "\n",
    "$\\theta^Tx >= 1$ if $y = 1$\n",
    "\n",
    "$\\theta^Tx <= -1$ if $y = 0$\n",
    "\n",
    "This objective function helps SVM achieve a decision boundary with a large margin.\n",
    "\n",
    "### What if SVM tries too hard to find a decision boundary in presence of outliers?\n",
    "We need to choose reasonably small value of $C$ so that SVM does not try too hard to accommodate for outliers\n",
    "\n",
    "\n",
    "### How SVM finds a decision boundary with a large margin?\n",
    "If $u$ and $v$ are two vectors $u^Tv$ is the length of projection of $v$ on $u$.\n",
    "Hence $\\theta^Tx$ is length of projection of $x$ on $\\theta$.\n",
    "With some mathematical jugglery the SVM objective function can be re-written as follows:\n",
    "\n",
    "$min_{\\theta} \\frac{1}{2}\\displaystyle\\sum_{j=1}^{n} ||\\theta||^2$\n",
    "\n",
    "such that\n",
    "\n",
    "$p^{(i)}||\\theta|| >= 1$ if $y = 1$\n",
    "\n",
    "$p^{(i)}||\\theta|| <= -1$ if $y = 0$\n",
    "\n",
    "where $p^{(i)}$ is the length of projection of $x^{(i)}$ on $\\theta$\n",
    "\n",
    "$\\theta$ is a characteristic vector of decision boundary such that it is orthogonal to the decision boundary.\n",
    "The SVM objective function forces the decision boundary to have a large margin since only in that case length of projections of individual data points $x$ is maximized and $||\\theta||$ is minimized.\n",
    "\n",
    "### Learning non linear decision boundary with SVM\n",
    "The hypothesis function for SVM we have discussed is linear since it depends on $\\theta^Tx$.\n",
    "Instead if we choose some landmarks and added feature $f^{(i)}_{k} = similarity(x^{(i)}, l_{k})$ where $l_{k}$ is a landmark.\n",
    "Hence the optimization objective of SVM for non-linear decision boundary is:\n",
    "\n",
    "$\\frac{C}{m}\\displaystyle\\sum_{i=1}^{m} y^{(i)}cost_{1}(h_\\theta(f^{(i)}) + (1- y^{(i)})cost_{0}(h_\\theta(f^{(i)}) + \\frac{1}{2m}\\displaystyle\\sum_{j=1}^{m} \\theta^2$ \n",
    "where \n",
    "\n",
    "$h_\\theta(x) = 1$ if $\\theta^Tf >= 0$\n",
    "\n",
    "$h_\\theta(x) = 0$ if $\\theta^Tf < 0$\n",
    "\n",
    "If $x^{(i)}$ is close to landmark $l_k$ then similarity will be close to 1 otherwise it will be close to 0.\n",
    "We choose our hypothesis as $\\theta^Tf >= 0$ if $y = 1$ and $\\theta^Tf < 0$ if $y = 0$ where $f$ is a feature vector based on landmarks we have chosen.\n",
    "This choice of non-linear similarity function (kernel) helps SVM learn non-linear decision boundary.\n",
    "\n",
    "#### What if we used similar technique in logistic regression? Can we learn non linear decision boundary using logistic regression?\n",
    "We can use similar features in `Logistic Regression`.\n",
    "However for training an SVM we use lot of computational tricks which do not generalize well for other learning alorithms. Generally training will be slow of other algorithms.\n",
    "\n",
    "### SVM in practice\n",
    "Many choices of kernels (similarity function) available.\n",
    "* Linear\n",
    "* Gaussian\n",
    "* Polynomial\n",
    "* String\n",
    "* Chi-square\n",
    "* Histogram intersection\n",
    "etc.\n",
    "The kernels need to satisfy *Mercer's Theorem*\n",
    "\n",
    "### Logistic Regression vs SVM\n",
    "* If we have many more features (10000 or more) than examples (1000 or less) use Logistic Regression (Linear Kernel).\n",
    "* If we have small number of features (1000 or less) and reasonable amount of data (10000 examples) use Gaussian Kernel.\n",
    "* If we have small number of features (1000 or less) and large data (50000+ examples) add more features and use Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM From ground up (the math and how it makes sense)\n",
    "Based on excellent [lecture](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "\n",
    "### How can we find a decision boundary that maximally separates positive and negative samples?\n",
    "\n",
    "#### What is the decision rule in such a thought process?\n",
    "Consider a vector $w$ perpendicular to the decision boundary.\n",
    "Consider another vector $u$ which we want to classify.\n",
    "$w^T . u$ is the projection of $u$ on $w$\n",
    "The decision rule is for a positive sample (data point) is:\n",
    "\\begin{equation}\n",
    "w^T . u + b >= 0\n",
    "\\end{equation}\n",
    "Otherwise it is a negative sample.\n",
    "\n",
    "We insist on the fact that for a positive sample $x_{+}$ the decision rule is:\n",
    "\\begin{equation}\n",
    "w^T . x_{+} + b >= 1\n",
    "\\end{equation}\n",
    "We want to be really sure about the positive sample.\n",
    "Similarly for a negative sample $x_{-}$ the decision rule is:\n",
    "\\begin{equation}\n",
    "w^T . x_{-} + b >= -1\n",
    "\\end{equation}\n",
    "\n",
    "For mathematical conviniece instead of 2 different conditions we write a single equation as:\n",
    "\\begin{equation}\n",
    "y_{i}(w^T . x_{i} + b) >= 1\n",
    "\\end{equation}\n",
    "Or equivalently:\n",
    "\\begin{equation}\n",
    "y_{i}(w^T . x_{i} + b) - 1 >= 0\n",
    "\\end{equation}\n",
    "\n",
    "Where $y_{i}$ is $1$ for positive sample and $-1$ for negative data sample.\n",
    "\n",
    "And for samples exactly on the margins of the decision boundary:\n",
    "\\begin{equation}\n",
    "y_{i}(w^T . x_{i} + b) = 0\n",
    "\\end{equation}\n",
    "\n",
    "#### Since we want the maximal separation between +ve and -ve samples we need a way to measure the separation between them. How can we do that?\n",
    "Let $x_{+}$ and $x_{-}$ be the samples on +ve margin and -ve margin respectively.\n",
    "The difference between them is $x_{+} - x_{-}$.\n",
    "If we had a unit vector perpendicular to the margins we can easily calculate the separation.\n",
    "We know $w$ is perpendicular to the decision boundary and hence the margins.\n",
    "Hence separation is calculated as:\n",
    "\\begin{equation}\n",
    "\\frac{w^T . (x_{+} - x_{-})}{||w||}\n",
    "\\end{equation}\n",
    "\n",
    "For a +ve sample on the boundary we know:\n",
    "\\begin{equation}\n",
    "y_{i}(w^T . x_{i} + b) = 0\n",
    "\\end{equation}\n",
    "\n",
    "Since $y_{i} = 1$\n",
    "\\begin{equation}\n",
    "w^T . x_{+} = 1 - b\n",
    "\\end{equation}\n",
    "\n",
    "Similarly from the -ve sample on the boundary: \n",
    "Since $y_{i} = -1$\n",
    "\\begin{equation}\n",
    "w^T . x_{-} = 1 + b\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Hence the separation is:\n",
    "\\begin{equation}\n",
    "\\frac{2}{||w||}\n",
    "\\end{equation}\n",
    "\n",
    "The goal is to maximize this separation which is equivalent to minimize $\\Vert{w}\\Vert$\n",
    "Hence we can minimize:\n",
    "\\begin{equation}\n",
    "    \\frac{||w||^2}{2}\n",
    "\\end{equation}\n",
    "\n",
    "Now we have an expression $\\frac{\\Vert{w}\\Vert^2}{2}$ which we want to minimize under constraints:\n",
    "\\begin{equation}\n",
    "y_{i}(w^T . x_{i} + b) = 0\n",
    "\\end{equation}\n",
    "For samples $(x_{i}, y_{i})$ that lie on the margin\n",
    "\n",
    "To do this we need to use **Langrange multipliers** which gives a new expression where we dont have to worry about the constraints.\n",
    "\n",
    "The new expression for our optimization problem will become as follows:\n",
    "\\begin{equation}\n",
    "    L = \\frac{||w||^2}{2} - \\displaystyle\\sum_{i=1}^{m} \\alpha_{i}[y_{i}(w^T . x_{i} + b) - 1]\n",
    "\\end{equation}\n",
    "\n",
    "To find the minima of this expression we will need derivatives which we can set to $0$.\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w} = w - \\displaystyle\\sum_{i=1}^{m} \\alpha_{i}y_{i}x_{i} = 0\n",
    "\\end{equation}\n",
    "Hence:\n",
    "\\begin{equation}\n",
    "w = \\displaystyle\\sum_{i=1}^{m} \\alpha_{i}y_{i}x_{i}\n",
    "\\end{equation}\n",
    "And:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial b} = - \\displaystyle\\sum_{i=1}^{m} \\alpha_{i}y_{i} = 0\n",
    "\\end{equation}\n",
    "Hence:\n",
    "\\begin{equation}\n",
    "\\displaystyle\\sum_{i=1}^{m} \\alpha_{i}y_{i} = 0\n",
    "\\end{equation}\n",
    "\n",
    "We plug these values in expression $L$ and we get:\n",
    "\\begin{equation}\n",
    "L = \\displaystyle\\sum_{i=1}^{m} \\alpha_{i} - (\\frac{1}{2})\\displaystyle\\sum_{i=1}^{m}\\displaystyle\\sum_{j=1}^{m} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i} . x_{j})\n",
    "\\end{equation}\n",
    "\n",
    "This is the final expression that we want to minimize using numerical methods.\n",
    "This is a *convex function* and hence has a global minima.\n",
    "The optimization only depends upon the `dot product` of the input samples like $x_{i}$ and $x_{j}$.\n",
    "Similarly the decision rule by plugging the value of $w$ becomes:\n",
    "\\begin{equation}\n",
    "    \\displaystyle\\sum_{i=1}^{m} \\alpha_{i}y_{i}(x_{i} . u) + b >= 0\n",
    "\\end{equation}\n",
    "which also depends upon only `dot product`.\n",
    "\n",
    "Once we know the optimal values of $\\alpha_{i}$'s we know the optimal values of $L$ and $w$.\n",
    "Using them we can get the optimal value of $b$.\n",
    "This gives us a linear decision boundary of SVM.\n",
    "\n",
    "### What if the data samples do not have a linear decision boundary? Kernel Trick\n",
    "We know the optimal value of $L$ depends upon $x_{i} . x_{j}$.\n",
    "We use a transformation function $\\phi$ to transform the input space.\n",
    "Now $L$ depends upon $\\phi(x_{i}) . \\phi(x_{j})$.\n",
    "Similarly for decision function need $\\phi(x_{i}) . \\phi(u)$\n",
    "\n",
    "If we have:\n",
    "\\begin{equation}\n",
    "K(x_{i}, x_{j}) = \\phi(x_{i}) . \\phi(x_{j})\n",
    "\\end{equation}\n",
    "Then we dont need $\\phi(x)$\n",
    "This function is known as *Kernel Function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "We assume all features of data are discrete valued.\n",
    "\n",
    "1. choose the best feature to classify data samples\n",
    "2. for each discrete value of the chosen feature create a new leaf node\n",
    "3. partition samples to newly created leaves with corresponding feature value\n",
    "4. iterate from step 1 each newly created leaf node if leaf node doesnt perfectly classify sampled\n",
    "\n",
    "### How to choose the best feature?\n",
    "Let $S$ be a set of examples of 2 classes: Positive $+$ and Negative $-$.\n",
    "Let $p_{1}$ is the fraction of samples of positive class and $p_{0}$ is the fraction of samples in the negative class.\n",
    "Thus $p_{1} + p_{0} = 1$.\n",
    "**Entropy** is the measure of impurity of $S$ and is defined as:\n",
    "\\begin{equation}\n",
    "    H(S) = -p_{1}log_{2}(p_{1}) - p_{0}log_{2}(p_{0})\n",
    "\\end{equation}\n",
    "If the data has equal proportion of both classes entropy is highest equal to $1$.\n",
    "If the data has samples from only one class the data is pure and entropy is lowest equal to $0$.\n",
    "Otherwise entropy is in $(0, 1)$.\n",
    "\n",
    "More generally entropy of a random variable $X$ is defined as:\n",
    "\\begin{equation}\n",
    "    H(X) = - \\sum_{i=1}^{n} p(X = i)log_2(P(X = i))\n",
    "\\end{equation}\n",
    "where $1,2...n$ are the distinct values that random variable $X$ may take.\n",
    "\n",
    "We define specific conditional entropy $H(X|Y=v)$ as:\n",
    "\\begin{equation}\n",
    "    H(X|Y=v) = - \\sum_{i=1}^{n} p(X = i|Y=v)log_2(P(X = i|Y=v))\n",
    "\\end{equation}\n",
    "\n",
    "Now conditional entropy $H(X|Y)$ is defined as:\n",
    "\\begin{equation}\n",
    "    H(X|Y) = - \\sum_{v \\in values(Y)} p(Y = v)H(X|Y=v)\n",
    "\\end{equation}\n",
    "\n",
    "Finally Information Gain is defined as:\n",
    "\\begin{equation}\n",
    "    I(X, Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n",
    "\\end{equation}\n",
    "\n",
    "Assuming all features in our data are discrete in decision tree algorithm we **choose the feature where we get maximum information gain**.\n",
    "Which is equivalent to choosing the feature with **minimum conditional entropy**.\n",
    "Each branch of the decision tree corresponds to a unique value of the chosen feature.\n",
    "At the end of each such branch we will get a collection of data samples which can be partitioned further based on other features.\n",
    "\n",
    "> If at a leaf node we find that the information gain obtained by splitting that leaf with any of the remaining attributes is negative we choose not to split that node. Since the leaf itself does a better job of classifying the example.\n",
    "\n",
    "#### What if the features are not discrete valued?\n",
    "Ideas obtained from [here](https://www.youtube.com/watch?v=7VeUPuFGJHk&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=35&t=0s)\n",
    "##### continuous valued attribute\n",
    "* Sort the data based on values of continuous valued attribute\n",
    "* Take average of adjacent values of the attributes\n",
    "* Choose these averages as splitting criteria\n",
    "* If there are n distinct values of continuous valued attribute we need to examine (n - 1) thresholds\n",
    "\n",
    "##### ranked data\n",
    "* use every rank as the threshold for splitting criteria\n",
    "\n",
    "##### categorical data\n",
    "* choose all possible subsets except $\\phi$ and subset containing all elements as the splitting criteria\n",
    "\n",
    "#### How to avoid decision trees from OVERFITTING?\n",
    "We can put a threshold on the information gain such that if the information gain is greater than that threshold only then we choose to split the leaf.\n",
    "Thus we obtain shorter tree.\n",
    "If we set this threshold too high then the decision tree will be very short and will be at the risk of underfitting.\n",
    "\n",
    "#### How does decision tree handle missing data?\n",
    "It can simply ignore the missing value since the impurity of the split is weighted on the number of samples in each individual leaf. This fact is captured in the definition of conditional entropy since it is weighted by $P(Y = v)$\n",
    "\n",
    "Alternatively we can find which other feature has a high correlation with current feature with missing value and use that feature to predict the missing value.\n",
    "Or we can use any other technique to impute such missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximation\n",
    "Given a set of inputs $X$ and a set of corresponding outputs $Y$ we want to learn the unknown function $f: X \\to Y$\n",
    "\n",
    "To do this we build a set of hypothesis functions $H = \\{ h | h: X \\to Y \\}$.\n",
    "We choose the hypothesis $h \\in H$ that best approximates $f$\n",
    "\n",
    "### How do we build hypothesis functions?\n",
    "### How to decide which hypothesis function is better?\n",
    "\n",
    "## Decision Trees\n",
    "We assume all features of data are discrete valued.\n",
    "\n",
    "1. choose the best feature to classify data samples\n",
    "2. for each discrete value of the chosen feature create a new leaf node\n",
    "3. partition samples to newly created leaves with corresponding feature value\n",
    "4. iterate from step 1 each newly created leaf node if leaf node doesnt perfectly classify sampled\n",
    "\n",
    "### How to choose the best feature?\n",
    "Let $S$ be a set of examples of 2 classes: Positive $+$ and Negative $-$.\n",
    "Let $p_{1}$ is the fraction of samples of positive class and $p_{0}$ is the fraction of samples in the negative class.\n",
    "Thus $p_{1} + p_{0} = 1$.\n",
    "Entropy is the measure of impurity of $S$ and is defined as:\n",
    "\\begin{equation}\n",
    "    H(S) = -p_{1}log_{2}(p_{1}) - p_{0}log_{2}(p_{0})\n",
    "\\end{equation}\n",
    "If the data has equal proportion of both classes entropy is highest equal to $1$.\n",
    "If the data has samples from only one class the data is pure and entropy is lowest equal to $0$.\n",
    "Otherwise entropy is in $(0, 1)$.\n",
    "\n",
    "More generally entropy of a random variable $X$ is defined as:\n",
    "\\begin{equation}\n",
    "    H(X) = - \\sum_{i=1}^{n} p(X = i)log_2(P(X = i))\n",
    "\\end{equation}\n",
    "where $1,2...n$ are the distinct values that random variable $X$ may take.\n",
    "\n",
    "We define specific conditional entropy $H(X|Y=v)$ as:\n",
    "\\begin{equation}\n",
    "    H(X|Y=v) = - \\sum_{i=1}^{n} p(X = i|Y=v)log_2(P(X = i|Y=v))\n",
    "\\end{equation}\n",
    "\n",
    "Now conditional entropy $H(X|Y)$ is defined as:\n",
    "\\begin{equation}\n",
    "    H(X|Y) = - \\sum_{v \\in values(Y)} p(Y = v)H(X|Y=v)\n",
    "\\end{equation}\n",
    "\n",
    "Finally Information Gain is defined as:\n",
    "\\begin{equation}\n",
    "    I(X, Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n",
    "\\end{equation}\n",
    "\n",
    "Assuming all features in our data are discrete in decision tree algorithm we **choose the feature where we get maximum information gain**.\n",
    "Which is equivalent to choosing the feature with **minimum conditional entropy**.\n",
    "Each branch of the decision tree corresponds to a unique value of the chosen feature.\n",
    "At the end of each such branch we will get a collection of data samples which can be partitioned further based on other features.\n",
    "\n",
    "#### What if the features are not discrete valued?\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "Decision Trees generally overfit.\n",
    "\n",
    "## How to build a random forest\n",
    "* Create a bootstrapped dataset od same size as original dataset by randomly selecting samples from original dataset. Same example my repeat multiple times.\n",
    "* Create a decision tree using only a subset of features at each step. The size of the subset of variables is a hyperparameter.\n",
    "* Repeat this procedure n times to get a random forest of size n.\n",
    "* For prediction use voting in between n created decision trees.\n",
    "\n",
    "This procedure of bootstrapping the dataset and aggregating the results is called **Bootstrap Aggregating** or **Bagging**.\n",
    "\n",
    "### Evaluation of random forests\n",
    "We evaluate a decision tree created during building a random forest using **Out-of-Bag accuracy**.\n",
    "That is we find the accuracy of the decision tree based on out of bag samples that were not present in creating that tree.\n",
    "Overall out of bag accuracy is the mean of out of bag accuracy of each decision tree.\n",
    "\n",
    "# Boosting\n",
    "\n",
    "## Adaboost\n",
    "* We are given a dataset and a weak learning algorithm A\n",
    "* In each iteration we construct a bag of samples from the original dataset based on the sample weight (initially each sample has equal weight)\n",
    "* We run algorithm A on this bag\n",
    "* We decrese the weight of correctly classified samples\n",
    "* We increase the weight on incorrectly classfied samples\n",
    "* Each of these weak learners will have a weight in terms of the error rate they have\n",
    "* The final hypothesis is the weighted sum of all of these intermediate hypotheses\n",
    "\n",
    "#### What is a weak learning algorithm?\n",
    "A weak learning algorithm does slightly better than random guessing.\n",
    "For a binary classification problem if its error rate $\\epsilon_{t} >= \\frac{1}{2} - \\gamma$ where $0 < \\gamma < \\frac{1}{2}$\n",
    "\n",
    "### Why not run the algorithm A on misclassified examples only?\n",
    "If we run the algorithm A on miclassified examples only then A will learn how to perfectly classify these misclassified examples.\n",
    "This algorithm has no idea about what it got right in the previous iteration.\n",
    "So potentially the model of the misclassified examples will opposite to the model that was learnt in the previous iteration.\n",
    "Hence adaboost puts half weight on correctly classified samples and half on incorrectly classified samples.\n",
    "This weight distribution is optimal to decrease the overall error rate to 0 as quickly as possible.\n",
    "\n",
    "### How do we construct the bag of samples? and How do we increase / decrease weight?\n",
    "Let $D_{t}(i)$ be the weight on the $i$th data sample in iteration $t$.\n",
    "Let $h_{t}$ be the hypothesis function learned in iteration $t$ based on algorithm $A$.\n",
    "Let $\\epsilon_{t}$ be the error rate of $h_{t}$ on samples in $D_t$.\n",
    "The the weight update is:\n",
    "\\begin{equation}\n",
    "    \\alpha_{t} = \\frac{1}{2} ln(\\frac{1 - \\epsilon_{t}}{\\epsilon_{t}})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    D_{t+1}(i) = \\frac{D_{t}(i)}{Z_{t}} e^{-\\alpha_{t}y_{i}h_{t}(x_{i})}\n",
    "\\end{equation}\n",
    "Here $y_{i} \\in \\{-1, 1\\}$ denoting the negative and positive class respectively.\n",
    "$Z_{t}$ is the normalization constant based on weights of all examples.\n",
    "\n",
    "### Advantages of Adaboost\n",
    "* Just need to find weak learning algorithm that do better than random guessing\n",
    "* No parameters to tune\n",
    "* Fast since only one pass through the data in each iteration\n",
    "* Well suited for big data age for distributed learning\n",
    "* Grounded in rich theory\n",
    "\n",
    "#### Training error of adaboost decreases exponentially with respect to number of iterations T\n",
    "Let $\\epsilon_{t} = \\frac{1}{2} - \\gamma_{t}$ the error of $h_t$ over $D_t$ then:\n",
    "\\begin{equation}\n",
    "    error_{S}(H_{final}) <= e^{-2 \\sum_{t}{} {\\gamma_{t}}^2}\n",
    "\\end{equation}\n",
    "If $\\forall \\ t: \\  0 < \\gamma <= \\gamma_{t}$ then $error_{S}(H_{final}) <= e^{-2T\\gamma^{2}}$\n",
    "\n",
    "#### Why final classifier has small error rate?\n",
    "If final classifier misclassifies a sample then a lot of intermediate weak learners would have misclassified this sample.\n",
    "Then at the end this misclassified example will have very large weight.\n",
    "Since weights over all examples are normalized not many examples will have  large weights over complete dataset.\n",
    "Such examples will be classified correctly.\n",
    "\n",
    "#### Disadvantages of Adaboost\n",
    "Sensitive to noise / outliers in the data.\n",
    "If the distribution puts weight on noisy examples then adaboost fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means clustering\n",
    "\n",
    "* Randomly choose k cluster centers\n",
    "* assign each point to the closest cluster center\n",
    "* update cluster centers to be the mean value of points associated with corresponding clusters\n",
    "* stop when cluster centers do not change much\n",
    "\n",
    "### Optimization objective\n",
    "\\begin{equation}\n",
    "    min \\ \\frac{1}{m} \\sum_{i=1}^{m} \\Vert x^{(i)} - \\mu_{c^{(i)}} \\Vert ^2\n",
    "\\end{equation}\n",
    "Here $m$ is number of data points, $\\mu_{c^{(i)}}$ is the cluster center assigned to $i$th data point $x^{(i)}$\n",
    "\n",
    "#### How to choose K cluster centers?\n",
    "Initialize cluster centers to be K random examples from the dataset.\n",
    "\n",
    "#### Random Initialization\n",
    "Such random initialization may get K means algorithm in local optima of optimization objective.\n",
    "To avoid local optima run K means algorithm multiple times and choose the best one based on lowest value of optimization objective.\n",
    "This random initialization will work only when number of clusters is relatively small $<= 10$.\n",
    "\n",
    "#### How to choose K?\n",
    "Generally chosen by hand.\n",
    "\n",
    "**Elbow Method**: Plot cost function against different values of K. From the point where there is no significant reduction in cost we can choose such K. May not work well if this plot is very smooth.\n",
    "\n",
    "Choose K based on later purpose of clusters (dependent on domain).\n",
    "\n",
    "#### Why K means converges?\n",
    "#### Why cost always drops in K means?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
