{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How to decide which algo performs better for our given dataset?\n",
    "* divide data into training and validation\n",
    "* train models and build confusion matrix\n",
    "\n",
    "#### True Positives\n",
    "* model correctly (truely) predicted as 1\n",
    "\n",
    "#### True Negatives\n",
    "* model correctly (truely) predicted as 0\n",
    "\n",
    "#### False Positives\n",
    "* model incorrectly (falsely) predicted as 1 (prediction 1 is incorrect)\n",
    "\n",
    "#### False Negatives\n",
    "* model incorrectly (falsely) predicted as 0 (prediction 0 is incorrect)\n",
    "\n",
    "#### Positives\n",
    "* model predicted as 1\n",
    "\n",
    "#### Negatives\n",
    "* model predicted as 0\n",
    "\n",
    "#### True\n",
    "* model correctly predicted\n",
    "\n",
    "#### False\n",
    "* model incorrectly predicted\n",
    "\n",
    "#### Sensitivity = Recall = (True Positive Rate)\n",
    "* what percentage of examples with label 1 were correctly classified by the model\n",
    "* proportion of correctly classified label 1 examples wrt number of 1s in the data\n",
    "* $\\frac{TP}{TP + FN}$\n",
    "* model with high sensitivity is better at predicting label 1\n",
    "\n",
    "#### Specificity\n",
    "* what percentage of examples with label 0 were correctly classified\n",
    "* $\\frac{TN}{TN + FP}$\n",
    "* model with high specificity is better at predicting label 0\n",
    "\n",
    "#### Model comparison\n",
    "* if it is more important to correctly predict label 1 then model with high sensitivity would be better\n",
    "* if it is more important to correctly predict label 0 then model with high specificity would be better\n",
    "\n",
    "#### Multi class metrics\n",
    "* for multi-class problems we need to calculate the above metric per class\n",
    "* the confusion matrix for n-class problem has n rows and n columns\n",
    "\n",
    "### How to decide which hyperparameter values are better?\n",
    "* Done with **ROC: Receiver Operator Characteristic** graphs\n",
    "* Y axis: **True Positive Rate = Sensitivity**\n",
    "* X axis: **False Positive Rate = 1 - Specificity**\n",
    "* $False\\ Positive\\ Rate = \\frac{FP}{FP+TN}$\n",
    "* ROC summarizes all confusion matrices\n",
    "\n",
    "### How to decide which ROC curve is better?\n",
    "* The ROC curve with more **AUC: Area under curve is better**\n",
    "\n",
    "#### Precision\n",
    "* proportion of 1 predictions that were correct out of all positive predictions\n",
    "* $\\frac{TP}{TP+FP}$\n",
    "* if there are lots of samples with label 0 precision may be a better metric than sensitivity since precision is unaffected by $TN$\n",
    "* so the imbalance in the dataset does not skew the metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Kernel Trick & How is it useful?\n",
    "* SVM optimization objective is dependent upon the dot product of individual samples $x_i.x_j$\n",
    "* SVM by default tries to find linear separation that maximizes margin\n",
    "* To find non-linear decision boundaries / hyperplanes we could transform feature vector $x_i$ via a non-linear function $\\phi$\n",
    "* Generally computing $\\phi(x)$ is compuationally expensive\n",
    "* Kernel $K(x_i, x_j) = \\phi(x_i)^T . \\phi(x_j)$ computes the dot product without the knowledge of $\\phi$ (hence the name kernel trick)\n",
    "* It allows us to operate in the original feature space without computing the coordinates of the data in a higher dimensional space.\n",
    "* when we map data to a higher dimension, there are chances that we may overfit the model. Thus choosing the right kernel function (including the right parameters) and regularization are of great importance.\n",
    "\n",
    "\n",
    "### SVM\n",
    "* We maximize the margin — the distance separating the closest pair of data points belonging to opposite classes. These points are called the support vectors, because they are the data observations that “support”, or determine, the decision boundary. To train a support vector classifier, we find the maximal margin hyperplane, or optimal separating hyperplane, which optimally separates the two classes in order to generalize to new data and make accurate classification predictions.\n",
    "\n",
    "https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f#:~:text=The%20%E2%80%9Ctrick%E2%80%9D%20is%20that%20kernel,the%20data%20by%20these%20transformed\n",
    "https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d\n",
    "\n",
    "### Can Kernel trick be used in any other algo?\n",
    "* in dual optimization objectives kernel trick can be used like in SVM\n",
    "* kernelized PCA, kernelized logistic regression\n",
    "\n",
    "### When should we use which kernel?\n",
    "https://stats.stackexchange.com/questions/18030/how-to-select-kernel-for-svm\n",
    "* a dataset with more than 2 dimensions? Here, we want to keep an eye on our objective function: minimizing the hinge-loss. We would setup a hyperparameter search (grid search, for example) and compare different kernels to each other. Based on the loss function (or a performance metric such as accuracy, F1, MCC, ROC auc, etc.) we could determine which kernel is \"appropriate\" for the given task.\n",
    "https://www.kdnuggets.com/2016/06/select-support-vector-machine-kernels.html\n",
    "\n",
    "### Why lasso does feature selection and why ridge does not?\n",
    "The consequence of this is that ridge regression will tend to shrink the large weights while hardly shrinking the smaller weights at all. In LASSO regression, the shrinkage will be directly proportionate to the importance of the feature in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression and $R^2$\n",
    "\n",
    "* When we fit a line to data using linear regression\n",
    "* The sum of error residuals is identified as:\n",
    "\\begin{equation}\n",
    "SS(fit) = \\sum_{i=1}^{m} (f(x) - y)^2\n",
    "\\end{equation}\n",
    "* the variation around the fitted line is:\n",
    "\\begin{equation}\n",
    "Var(fit) = \\frac{SS(fit)}{m}\n",
    "\\end{equation}\n",
    "\n",
    "* the sum of squares around mean value of $y$ is:\n",
    "\\begin{equation}\n",
    "SS(mean) = \\sum_{i=1}^{m} (y_{mean} - y)^2\n",
    "\\end{equation}\n",
    "* the variation around the fitted line is:\n",
    "\\begin{equation}\n",
    "Var(mean) = \\frac{SS(mean)}{m}\n",
    "\\end{equation}\n",
    "\n",
    "* $R^2$ is defined as:\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{Var(mean) - Var(fit)}{Var(mean)}\n",
    "\\end{equation}\n",
    "* alternatively\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{SS(mean) - SS(fit)}{SS(mean)}\n",
    "\\end{equation}\n",
    "since the denominator cancels out\n",
    "\n",
    "* predicting mean value of y always means $R^2$ is 0 hence no features explain variation in $y$\n",
    "* when $R^2$ is 1 i.e. $Var(fit) = 0$ mean features fully explain variation in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
