{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical ML\n",
    "Source: Andrew Ng Coursera [lectures](https://www.coursera.org/learn/machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate hypothesis?\n",
    "\n",
    "* Ensure **no ordering** in the data. Otherwise shuffle data.\n",
    "* Split the data in train-test set. Generally 70-30 split.\n",
    "* Use train data for learning and compute error on test set.\n",
    "* For classification problems we can also compute misclassification error. What fraction of examples did the model get wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Which Hypothesis function(s) we should pursue more?\n",
    "* The goal of a ML model is to generalize well on **unseen data**\n",
    "* If we use the test data to select model then we choose the best model that fits the test data. Such model may not generaliza well on real unseen data.\n",
    "* Instead split the data into train-validation-test set with 60-20-20 split in general\n",
    "* Use `validation` set to optimize parameters and hyperparameters of the model\n",
    "\n",
    "**Use Test set only once to report true generalization capabilities of the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance tradeoff\n",
    "* As model complexity increases training error reduces\n",
    "* Initially for simple models validation error is high (**underfitting** / **high bias**)\n",
    "* This is evident via **high training error** and **high validation error**\n",
    "* As model complexity increases then validation error decreases\n",
    "* After a point as model complexity increases more validation error increases (**overfitting / high variance**)\n",
    "* This is evident via **low training error** and **high validation error**\n",
    "\n",
    "### Bias-Variance with respect to regularization\n",
    "* If we regularize too much then underfitting / high bias\n",
    "* If we regularize too less then overfitting / high variance\n",
    "\n",
    "#### Choosing regularization parameter\n",
    "* Define train, validation and test errors wrt optimization objective of the model without the regularization term\n",
    "* For differe values of regularization parameters find validation error and choose the regularization parameter that minimizes validation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing High Bias or High Variance via Learning curves\n",
    "* Plot error against training set size\n",
    "* Generally as size of training set increases training error increases since for less no. of examples it is easy to fit to data\n",
    "* As training set size increases validation error starts to decrease since the more training data we have the better are our generalization guarantees\n",
    "\n",
    "\n",
    "* If the model suffers from **high bias** even if we increase training set size validation error will not decrease beyond a certain point\n",
    "* Similarly the training error will increase and after a point will not increase much and will be very close to validation error\n",
    "* This results in high training error and high validation error with very similar values\n",
    "* So for a model suffering from high bias **getting more data is NOT going to help much**\n",
    "\n",
    "\n",
    "* If the model suffers from **high variance** as training set size increase training error increases but not very much since model fit training data well\n",
    "* Validation error starts to decrease as training set size increases but does not decrease very much since model does not generalize well\n",
    "* The validation error will be significantly higher than training error\n",
    "* In this case **getting more training data WILL help**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do if a given ML model performs poorly?\n",
    "* **Collect More Data**: fixes **HIGH VARIANCE**\n",
    "* **Try less no. of features**: fixes **HIGH VARIANCE**\n",
    "* **Get more no. of features**: fixes **HIGH BIAS**\n",
    "* **Add new features from existing data**: fixes **HIGH BIAS**\n",
    "* **Decrease regularization**: fixes **HIGH BIAS**\n",
    "* **Increase regularization**: fixes **HIGH VARIANCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritizing what to work on about improving algorithm\n",
    "* Get more data\n",
    "* Build better features\n",
    "* Build algorithms to solve subproblems\n",
    "\n",
    "### Error Analysis\n",
    "* Implement simple algorithm\n",
    "* Plot learning curves to find high bias or high variance\n",
    "* Error analysis: manually look at data that algorithm misclassified and try to find a pattern in them\n",
    "* Try to categorize data based on various parameters and find insights where algorithm is making more mistakes\n",
    "* Find what features can help to classify such things correctly\n",
    "\n",
    "### Single evaluation numerical metric\n",
    "* Helps to evaluate ideas quickly\n",
    "* Evaluate performance of algrithm with and without idea and see if it helps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling skewed classes\n",
    "* Only having accuracy in presence of skewed class data is not the correct approach since we can jus always predict the majority class and get really good accuracy\n",
    "* Precision and Recall in such case are better metrics\n",
    "* Since we need a single number evaluation metric we can use f1-score\n",
    "\n",
    "Note: use StatQuest channel to expand more on precision recall f1-score even in multi-class settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tradeoff between precision and recall\n",
    "* If model tends to predict negative more often than positive then it wants to be really sure (strict criteria) about positive. Such model will have high precision and low recall.\n",
    "* If model tends to predict positive more often then it has very relaxed criteria. Such model has high recall and low precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large data in ML\n",
    "* The more data we give it to a learning algorithm the better the results will be\n",
    "* With a large training set actual algorithm does not matter as much if it has sufficient number of parameters\n",
    "\n",
    "* Use an algorithm has many parameters. Leads to low training error. This takes care of high bias.\n",
    "* Provide large training data. This is unlikely to overfit. \n",
    "* This causes test error to be very close to training error. Since number of training examples is much more than number of features. Hene model will have low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with large datasets\n",
    "\n",
    "* Classical gradient descent will be very slow since we compute average over all examples\n",
    "* In such case it is better to use **stochastic gradient descent**\n",
    "* Another alternative: **minibatch gradient descent** use only subset of examples for gradient descent update\n",
    "* minibatch gradient descent is faster than gradient descent\n",
    "* minibatch gradient descent is faster than stochastic gradient descent if we use vectorized operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
